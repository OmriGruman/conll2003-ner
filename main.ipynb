{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lzvrOf5fKZKy"
      },
      "source": [
        "# Named Entity Recognition: RNN vs Pre-Trained Transformer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SKCYvMgxKZKz"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "RbCGrBgIKZK0"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# GloVe\n",
        "import gensim.downloader\n",
        "\n",
        "# DL\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hugging Face\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a8QnxHZ4KZK2"
      },
      "source": [
        "## 1 RNN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UlBcI2CuKZK2"
      },
      "source": [
        "### 1.1 Data Exploration & Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "glove_embeddings = gensim.downloader.load(\"glove-wiki-gigaword-300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(400000, 300)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(glove_embeddings), len(glove_embeddings[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading builder script: 100%|██████████| 9.57k/9.57k [00:00<00:00, 9.56MB/s]\n",
            "Downloading metadata: 100%|██████████| 3.73k/3.73k [00:00<00:00, 1.80MB/s]\n",
            "Downloading readme: 100%|██████████| 12.3k/12.3k [00:00<00:00, 6.17MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset conll2003/conll2003 to C:/Users/markk/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data: 100%|██████████| 983k/983k [00:00<00:00, 2.29MB/s]\n",
            "                                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset conll2003 downloaded and prepared to C:/Users/markk/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 70.49it/s]\n"
          ]
        }
      ],
      "source": [
        "conll2003_dataset = load_dataset(\"conll2003\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[14041, 3250, 3453]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[dataset.num_rows for dataset in conll2003_dataset.values()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9\n",
            "9\n"
          ]
        }
      ],
      "source": [
        "conll2003_dataset['train'].features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'O': 0,\n",
              "  'B-PER': 1,\n",
              "  'I-PER': 2,\n",
              "  'B-ORG': 3,\n",
              "  'I-ORG': 4,\n",
              "  'B-LOC': 5,\n",
              "  'I-LOC': 6,\n",
              "  'B-MISC': 7,\n",
              "  'I-MISC': 8},\n",
              " {0: 'O',\n",
              "  1: 'B-PER',\n",
              "  2: 'I-PER',\n",
              "  3: 'B-ORG',\n",
              "  4: 'I-ORG',\n",
              "  5: 'B-LOC',\n",
              "  6: 'I-LOC',\n",
              "  7: 'B-MISC',\n",
              "  8: 'I-MISC'})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tag2id = {tag: i for i, tag in enumerate(conll2003_dataset['train'].features['ner_tags'].feature.names)}\n",
        "id2tag = {i: tag for i, tag in enumerate(conll2003_dataset['train'].features['ner_tags'].feature.names)}\n",
        "\n",
        "tag2id, id2tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_tokens = []\n",
        "\n",
        "for sequence in conll2003_dataset['train']:\n",
        "    all_tokens = list(set(all_tokens + sequence['tokens']))\n",
        "    \n",
        "print(len(all_tokens))\n",
        "\n",
        "all_tokens = []\n",
        "\n",
        "for sequence in conll2003_dataset['validation']:\n",
        "    all_tokens = list(set(all_tokens + sequence['tokens']))\n",
        "    \n",
        "print(len(all_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_len = 500\n",
        "\n",
        "def get_keys_from_DataSet(DS_NAME, keys = ['tokens','ner_tags']):\n",
        "     return [conll2003_dataset[DS_NAME][:max_len].get(key) for key in keys]\n",
        " \n",
        "# Split data\n",
        "train_data, train_label =get_keys_from_DataSet('train')\n",
        "val_data, val_label= get_keys_from_DataSet('validation')\n",
        "test_data, test_label  = get_keys_from_DataSet('test')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HbOtr-9DKZK6"
      },
      "source": [
        "### 1.2 Dataset & Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data\n",
        "vocab = []\n",
        "for line in train_data:\n",
        "  vocab += line\n",
        "vocab+= ['<unk>']\n",
        "\n",
        "# Create dictionaries to convert between tokens and indices\n",
        "token_to_index = {tok: i for i, tok in enumerate(set(vocab))}\n",
        "index_to_token = {i: tok for i, tok in enumerate(set(vocab))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ToDataset(data, label):\n",
        "    return Dataset.from_dict({\n",
        "    \"input\": data,\n",
        "    \"labels\":  label\n",
        "})\n",
        "    \n",
        "train_dataset = ToDataset(train_data, train_label)\n",
        "val_dataset =ToDataset(val_data, val_label)\n",
        "test_dataset =ToDataset(test_data, test_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ToDataloader(Dataset, batch_size = 2, shuffle = False, num_workers = 4):\n",
        "    return DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=shuffle,\n",
        "    num_workers=num_workers)\n",
        "\n",
        "\n",
        "# Create a DataLoader object\n",
        "train_dataloader = ToDataloader(train_dataset)\n",
        "val_dataloader = ToDataloader(val_dataset)\n",
        "test_dataloader = ToDataloader(test_dataset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qd9NW6F4KZK7"
      },
      "source": [
        "### 1.3 Model "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zxcX4EVWw6C6"
      },
      "source": [
        "#### 1.3.1 Definition"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-CsPUE1pw6C7"
      },
      "source": [
        "#### 1.3.2 Experiments"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.3.3 Evaluation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a3Im7yxEKZLP"
      },
      "source": [
        "## 2 Pre-Trained Transformer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t8m7k9U5KZLP"
      },
      "source": [
        "### 2.1 Dataset & Dataloader"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EZExQ5fZuq1d"
      },
      "source": [
        "### 2.2 Fine-Tuning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bAcdz9yYKZLS"
      },
      "source": [
        "#### 2.3 Comparison to RNN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.3.1 Setups"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.3.2 Approaches"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.3.3 Error Analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 Conclusions"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
